{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Experiment Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be running the MDP_Model on the Maze problem. This 2D simulation builds on a `gym-maze` package that can be found here: https://github.com/MattChanTK/gym-maze. Before beginning this simulation, please be sure to install the relevant packages on the github **Installation** section (pygame and numpy are also required)!\n",
    "\n",
    "If you haven't done so already, you can run the following cell. It it recommended to start a new python environment before doing so, to avoid any incompatibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing general modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append system paths\n",
    "import sys\n",
    "sys.path.append(\"../gym-maze/\")\n",
    "sys.path.append(\"../mrl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "import gym_maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a quick demonstration about what the gym environment we use. Essentially, there is an agent that starts on the upper left cell, then keeps taking steps in the maze until it reaches the end point. Here is a simulation to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the gym environment for a maze\n",
    "\n",
    "env = gym.make(\"maze-sample-5x5-v0\")\n",
    "\n",
    "# Set number of iterations\n",
    "n_iterations = 20\n",
    "\n",
    "# Running the maze\n",
    "observation = env.reset()\n",
    "for _ in range(n_iterations):\n",
    "    try:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "          observation = env.reset()\n",
    "    except ValueError:\n",
    "        pass\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Maze environment\n",
    "\n",
    "Here, we will observe paths of the agent walking through the Maze. Each time the agent is in a cell, we observe only a random 2D point in the cell and a rewards (here denoted RISK, $=-1/A$, where $A$ is the number of cells, except in the goal cell $= 1$). At each step, the agent can take action up, down, left, right and will transition to a random 2D point in the next cell, unless there is wall. Then, it stays in the same cell, within a new random 2D points. In particular, we do not know the cells, walls, or dynamics of the Maze. We only observe paths. The goal is to learn the structure of the Maze, which is the minimal representation of this continuous state space MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actions are encoded as follows:\n",
    "actions = {0: 'Up', 1: 'Down', 2: 'Right', 3: 'Left'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading relevant packages and functions - make sure to change the `sys.path.append` line with the relevant directory that contains the MDP Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing MRL specific libraries\n",
    "from mdp_utils import *\n",
    "from clustering import *\n",
    "from model import MDP_model\n",
    "from maze_functions import createSamples, opt_maze_trajectory, opt_model_trajectory, policy_accuracy, \\\n",
    "    get_maze_transition_reward, plot_paths\n",
    "from testing import cluster_size, next_clusters, training_value_error, purity, plot_features, testing_value_error\n",
    "\n",
    "mazes = {1: 'maze-v0',\n",
    "         2: 'maze-sample-3x3-v0',\n",
    "         3: 'maze-random-3x3-v0',\n",
    "         4: 'maze-sample-5x5-v0',\n",
    "         5: 'maze-random-5x5-v0',\n",
    "         6: 'maze-sample-10x10-v0',\n",
    "         7: 'maze-random-10x10-v0',\n",
    "         8: 'maze-sample-100x100-v0',\n",
    "         9: 'maze-random-100x100-v0',\n",
    "         10: 'maze-random-10x10-plus-v0', # has portals \n",
    "         11: 'maze-random-20x20-plus-v0', # has portals \n",
    "         12: 'maze-random-30x30-plus-v0'} # has portals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Samples\n",
    "\n",
    "Now selecting the environment parameters: here, we decide how many paths in the maze we generate (`N`), the number of steps (horizon) of each path (`T_max`), and the maze that we want the agent to run through.\n",
    "\n",
    "`reseed`' is set to `True` iif we change the randomness seed every path.\n",
    "\n",
    "`r` dictates the action policy of the agent. At each step, the agent takes a step in the optimal direction towards the goal with probability $1-r$ and a random direction with probability $r$.\n",
    "\n",
    "We select here a $5\\times 5$ Maze, generate $200$ paths, each of horizon $50$, with a policy that takes half of the times the optimal direction and a random direction the other half. You can verify here the rewards are $-0.04 = -\\frac{1}{5\\times 5}$ for non-goal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Parameters\n",
    "N = 200\n",
    "T_max = 50\n",
    "r = 0.5\n",
    "maze = mazes[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createSamples(N, T_max, maze, r, reseed=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `FEATURE_0` and `FEATURE_1` are the `x` and `y` coordinates respectively, while `ACTION` corresponds to the {0: 'Up', 1: 'Down', 2: 'Right', 3: 'Left'} directions. `RISK` is the reward (`1` if the endstate goal is reached, otherwise $-1/A$, with $A$ being the size of the maze)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how the transition data looks like (dashed lines are not visible to the algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_paths(df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the `N` generated paths, here's how many reached the goal state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ACTION']=='None']['ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting to Algorithm (Cross Validation (CV) Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the algorithm on the generate data set.\n",
    "This is an example of training with cross validation! For faster training, simply change `m.fit_CV` to `m.fit`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a breakdown of MRL parameters:\n",
    "- `max_k`: a cap of the number of clusters (states) we allow. *Further Notes:* This will then be am upper bound on the number of splits MRL does (and therefore on runtime). In this environment, the initial clustering, base on rewards will lead two clusters (end state and all others). The expected optimal `max_k` should be then be larger than the total number of cells of the Maze to be able to recover the minimal representation.\n",
    "- `pfeatures`: how many features we have in the dataset, in this case 2 (coordinates x and y). \n",
    "- `classification`: represents the type of classifier we want to use when splitting clusters. Options for this classifier include `'DecisionTreeClassifier'`, `'LogisticRegression'`, `'RandomForestClassifier'`, `'MLPClassifier'`, and `'AdaBoostClassifier'`. *Further Notes:* Think of this as the hypothesis class. In our experiment decision trees work better.\n",
    "- `split_classifier_params`: passes in the arguments necessary to the split classifier. *Further Notes:* For example for decision trees, this includes random_state and max_depth. \n",
    "- `clustering`: indicates the method used to form the initial clusters (based on rewards), with options of `'Agglomerative'`, `'KMeans'`, or `'Birch'`. \n",
    "- `n_clusters`: can be passed in if we want to fix the number of initial clusters (based on rewards). This is useful in the case of non-discrete rewards. In the case of discrete rewards, as for the Maze env, we typically use `'Agglomerative'` cluster. In this case a `distance_threshold` must also be passed in to determine the minimal distance between the rewards of two clusters (read `sklearn.cluster` documentation on `AgglomerativeClustering` for more details). \n",
    "- `precision_thresh`: determines the minimum decrease in value error necessary for the model to determine that a split gives a better clustering/representation.  *Further Notes:* This value attempts to limit model complexity when improvements becomes only incremental.\n",
    "- `eta`: sets a maximum incoherence threshold for a valid representation. *Further Notes:* Incoherence is defined as the the number of points in each cluster-action pair that do not go to the majority next cluster (transition) observed in the data; see definition in the paper. During training, any clustering/representation that results in a maximum incoherence above `eta*sqrt(n)/c`, where `n` is the number of data points given and `c` is the number of clusters at this current split, will be disregarded as too incoherent when selecting the number of clusters for the learnt representation.\n",
    "- `th`: is a threshold on the number of incoherences in a cluster/state to trigger a split.\n",
    "- `gamma`: is the MDP discount factor used when calculating values (in particular evaluating value error).\n",
    "- `h` is the number of time steps (horizon) on which to evaluate the value error. When `h = -1`, we evaluate over an infinite horizon, the whole path.\n",
    "- `cv` is the number of folds for cross validation, only relevant when you run `model.fit_CV()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for model fitting\n",
    "max_k = 20\n",
    "pfeatures = 2\n",
    "classification = 'DecisionTreeClassifier'\n",
    "split_classifier_params = {'random_state':0, 'max_depth':2}\n",
    "clustering = 'Agglomerative'\n",
    "n_clusters = None\n",
    "distance_threshold = 0.5 #for Agglomerative clustering\n",
    "random_state = 0\n",
    "h = -1\n",
    "gamma = 1\n",
    "cv = 5\n",
    "th = 0\n",
    "eta = 25\n",
    "precision_thresh = 1e-14\n",
    "\n",
    "m = MDP_model()\n",
    "m.fit_CV(df, # df: dataframe in the format ['ID', 'TIME', ...features..., 'RISK', 'ACTION']\n",
    "    pfeatures, # int: number of features\n",
    "    h, # int: time horizon (# of actions we want to optimize)\n",
    "    gamma, # discount factor\n",
    "    max_k, # int: number of iterations\n",
    "    distance_threshold, # clustering diameter for Agglomerative clustering\n",
    "    cv, # number for cross validation\n",
    "    th, # splitting threshold\n",
    "    eta, # incoherence threshold, calculated by eta*sqrt(datapoints)/clusters\n",
    "    precision_thresh, # precision threshold\n",
    "    classification, # classification method\n",
    "    split_classifier_params, # classifier params\n",
    "    clustering,# clustering method from Agglomerative, KMeans, and Birch\n",
    "    n_clusters, # number of clusters for KMeans\n",
    "    random_state,\n",
    "    plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the learnt MRL representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us see what MDP states/clusters MRL learnt, and compare it with the actual cells of the Maze.\n",
    "We make a special class `Maze_Model_Visualizer` to visualize various aspects of a learnt MRL's maze representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_functions import Maze_Model_Visualizer\n",
    "vis = Maze_Model_Visualizer(m)\n",
    "\n",
    "vis.plot_features()\n",
    "vis.plot_features(rep = 'OG_CLUSTER', title = 'Cells of the Maze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize a given cluster and its learnt transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_features() #we first create the figure to show the cluster in\n",
    "#Show a given cluster\n",
    "vis.show_cluster(cluster = 3, color = 'black')\n",
    "#Show a given point\n",
    "vis.show_point(point = (2.5,-2.2), color = 'red', s=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the constructed states (discretization), we can now build an MDP model (see paper) and use it to predict values and learn an optimal policy.\n",
    "\n",
    "When building the MDP model from the discretization, we incorporate some robustness checks with following paramters: `min_action_obs`, `min_action_purity`, `alpha`, `beta`. When we observe data transitions from state $s$ to $s'$ under action $a$, a transition in built in the MDP only if it passes the following tests:\n",
    "- *Minimal Number of Observations:* Number of observations $s \\rightarrow_a s'$ is $\\geq$ `min_action_obs`.\n",
    "- *Minimal Ratio of Agreement in the State:* Among the data points in state $s$, taking action $a$, at least a ratio of `min_action_purity` indeed transitioned to $s'$.\n",
    "- *Hypothesis test:* We assume null hypothesis that $\\leq$ `beta` ratio of our data $(s,a)$ is actually going into $s'$, and seek to reject this hypothesis with significance p<=`alpha`. If we fail to reject, then we remove this cluster-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we build an MDP representation and compute its optimal policy\n",
    "m.solve_MDP(min_action_obs=-1, alpha = 0.2, beta = 0.85, min_action_purity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the constructed MDP with these tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now visualize a given transition\n",
    "#origine cluster is in black, and the one it transition to is in red\n",
    "vis.plot_features()\n",
    "print('Transition for action', actions[1])\n",
    "vis.show_transition(cluster=10,action=1)\n",
    "#Observe the details how the constructed transition (# of points, purity, etc)\n",
    "vis.transition_details(cluster=10,action=1)\n",
    "#and now visualize the learnt optimal policy\n",
    "vis.policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now simulate a path in the Maze with the learnt policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.simulate_opt_policy(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate how well the model predict values of unseen data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = createSamples(100, T_max, maze, 0.3, reseed=True) #new unseen trajectories\n",
    "val_error = testing_value_error(df_test, m.df_trained, m.m, m.pfeatures, relative=False, h=-1); #value error on these points\n",
    "print('Value error on test set:', val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and how well it learns the states of the minimal representation of the Maze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MRL learnt states accuracy, as compared with Maze cells:', m.clus_pred_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance as a Function of Data Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now investigate how well MRL recovers the minimal representation with growing data size $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "N = 200 #Max data size\n",
    "T_max = 25 #Max number of steps in each trajectory\n",
    "r = 0.5 #percentage of optimal action is data generation policy\n",
    "maze = mazes[4]\n",
    "\n",
    "df = createSamples(N, T_max, maze, r, reseed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to see how well the model trains on different subsets of the data, starting with when it only has access to the first 10 paths, `N=10`, all the way to all 200 paths, `N=200`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# Setting MRL params\n",
    "max_k = 25\n",
    "classification = 'DecisionTreeClassifier'\n",
    "split_classifier_params = {'random_state':0, 'max_depth':2}\n",
    "clustering = 'Agglomerative'\n",
    "n_clusters = None\n",
    "distance_threshold = 0.5\n",
    "random_state = 0\n",
    "pfeatures = 2\n",
    "gamma = 1\n",
    "actions = [0, 1, 2, 3]\n",
    "h = -1\n",
    "cv = 5\n",
    "th = 0\n",
    "eta = 25\n",
    "precision_thresh = 1e-14\n",
    "\n",
    "#Data size\n",
    "Ns = [10, 20, 30, 40, 50, 70, 90, 110, 130, 150, 170, 200]\n",
    "df_full = df.copy()\n",
    "\n",
    "models=[]\n",
    "\n",
    "# Training models \n",
    "for n in Ns:\n",
    "    df_small = df_full.loc[df_full['ID']<n]\n",
    "    \n",
    "    m = MDP_model()\n",
    "    print('Training for N=', n, ' .....')\n",
    "    m.fit(df_small, # df: dataframe in the format ['ID', 'TIME', ...features..., 'RISK', 'ACTION']\n",
    "        pfeatures, # int: number of features\n",
    "        h, # int: time horizon (# of actions we want to optimize)\n",
    "        gamma, # discount factor\n",
    "        max_k, # int: number of iterations\n",
    "        distance_threshold, # clustering diameter for Agglomerative clustering\n",
    "        cv, # number for cross validation\n",
    "        th, # splitting threshold\n",
    "        eta, # incoherence threshold\n",
    "        precision_thresh, # precision threshold\n",
    "        classification, # classification method\n",
    "        split_classifier_params, # classification params\n",
    "        clustering,# clustering method from Agglomerative, KMeans, and Birch\n",
    "        n_clusters, # number of clusters for KMeans\n",
    "        random_state,\n",
    "        plot=False,\n",
    "        optimize=True)\n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate training and testing value error as a function of data size MRL was trained on. As a reminder these errors are computed by how well can MRL's model predict the value of a given path in the continuous state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[0]['FEATURE_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import testing_value_error\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "# Creating a test set with same parameters as training set\n",
    "N = 200\n",
    "T_max = 25\n",
    "r = 0.5\n",
    "maze = mazes[4]\n",
    "df_test = createSamples(N, T_max, maze, r, reseed=True)\n",
    "\n",
    "# training and testing value errors: \n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "for m in models: \n",
    "    tr_err = m.training_error.loc[m.training_error['Clusters']==m.opt_k]['Error'].min()\n",
    "    te_err = testing_value_error(df_test, m.df_trained, m.m, m.pfeatures, gamma, relative=False, h=-1)\n",
    "    training_errors.append(tr_err)\n",
    "    testing_errors.append(te_err)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(Ns, training_errors, label='Training Error')\n",
    "ax1.plot(Ns, testing_errors, label='Testing Error')\n",
    "ax1.set_title('Testing and Training Errors by N')\n",
    "ax1.set_xlabel('N training data size')\n",
    "ax1.set_ylabel('Error')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to measure both training and testing state classification accuracies, measured as how well the model learns and maps each point to the correct original cell of the maze, which correspond to the minimal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import generalization_accuracy\n",
    "\n",
    "tr_acc, test_acc = generalization_accuracy(models, df_test, Ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure here the value optimality gap: Given a point that start randomly in the starting state, we want to difference between the total collected value by the optimal policy and the collected value by MRL's model's prescribed policy. When simulating each policy, the point progresses through the maze according to the actual maze dynamics; the only difference lies in what sequence of actions it is given.\n",
    "\n",
    "We will once again plot the change in this value as data size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_functions import get_maze_MDP, get_maze_transition_reward, value_diff\n",
    "\n",
    "random.seed(0)\n",
    "# Set Parameters\n",
    "P, R = get_maze_MDP(maze)\n",
    "K = 100 #number of simulation of the found policy to estimate its expected value\n",
    "f, rw = get_maze_transition_reward(maze)\n",
    "\n",
    "betas = np.array(Ns)/max(Ns) *0.6+0.3\n",
    "for i,model in enumerate(models):\n",
    "    model.solve_MDP(min_action_obs=-1, alpha = 0.2, beta = betas[i])\n",
    "\n",
    "opt_gap = value_diff(models, Ns, K, T_max, P, R, f, rw)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(Ns, opt_gap)\n",
    "ax1.set_title('Optimality Gap by Data Size N')\n",
    "ax1.set_xlabel('N training data size')\n",
    "ax1.set_ylabel('|V_alg-V*|')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
